{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"2a161d00be9445b18b20037654d312da":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7842dccc547c47b7b7c9235b92e9b9cc","IPY_MODEL_85c7a13d776f44958290e1ed063271b6","IPY_MODEL_f6cf07f904fd424d88695d586b8d2cbf"],"layout":"IPY_MODEL_db2b464bb6174df19771c9f80917d360"}},"7842dccc547c47b7b7c9235b92e9b9cc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_528c26b7f6a34ac6b33057e6a2732239","placeholder":"​","style":"IPY_MODEL_ffb4fab85eb243a4b00e59f6ba8bf216","value":"tokenizer_config.json: 100%"}},"85c7a13d776f44958290e1ed063271b6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d7116354d584caa9581914424ad47e1","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_05a4af6769a94addb4647594f9bd340d","value":28}},"f6cf07f904fd424d88695d586b8d2cbf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c8c3db074c98444681f7a96929b4ba8e","placeholder":"​","style":"IPY_MODEL_115dac200f474c5095555157653481de","value":" 28.0/28.0 [00:00&lt;00:00, 1.27kB/s]"}},"db2b464bb6174df19771c9f80917d360":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"528c26b7f6a34ac6b33057e6a2732239":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ffb4fab85eb243a4b00e59f6ba8bf216":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4d7116354d584caa9581914424ad47e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"05a4af6769a94addb4647594f9bd340d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c8c3db074c98444681f7a96929b4ba8e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"115dac200f474c5095555157653481de":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e805a6d19bb043aaa205684986762bed":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_228b9673420b451bac630a7933262c27","IPY_MODEL_50cf265ca2344bbe8a41af04289a9f3d","IPY_MODEL_5c86dba6dfe94ad88e2b292b802e84e4"],"layout":"IPY_MODEL_e991d282d3214ffdba3b659ecd8412a0"}},"228b9673420b451bac630a7933262c27":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_565d3b4b5d4a43ff8ca933d4216fe410","placeholder":"​","style":"IPY_MODEL_77e215d6bf314be88d213b0ccc3d3840","value":"vocab.json: 100%"}},"50cf265ca2344bbe8a41af04289a9f3d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6360fedd21b64dc694dea7c85db38d76","max":1561415,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3eb185b4bf034d5abd1ec8d9558f486a","value":1561415}},"5c86dba6dfe94ad88e2b292b802e84e4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_93cdba5bcbbf4a17b5da785245a7d79f","placeholder":"​","style":"IPY_MODEL_1d2f787652a34bd58872002300ed6dbe","value":" 1.56M/1.56M [00:00&lt;00:00, 6.20MB/s]"}},"e991d282d3214ffdba3b659ecd8412a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"565d3b4b5d4a43ff8ca933d4216fe410":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"77e215d6bf314be88d213b0ccc3d3840":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6360fedd21b64dc694dea7c85db38d76":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3eb185b4bf034d5abd1ec8d9558f486a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"93cdba5bcbbf4a17b5da785245a7d79f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d2f787652a34bd58872002300ed6dbe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ec46ccd7fa9041b3812609079bf25227":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_40fc7737d0e84ac79f5a94512db208f3","IPY_MODEL_1f93d426cba64c9088b1fa8bbf6066b5","IPY_MODEL_28763562fc134c9ba738fbe1a2cf8735"],"layout":"IPY_MODEL_25f758d718a94f46a731107b5617d927"}},"40fc7737d0e84ac79f5a94512db208f3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_482792b7827d475fab977a2b85bb2f30","placeholder":"​","style":"IPY_MODEL_5eb309639cd64de0ab847ef4679114f8","value":"merges.txt: 100%"}},"1f93d426cba64c9088b1fa8bbf6066b5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5796140660234497aebc662697065c5f","max":895731,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3ca5fcbb25da4a0cb7057c4207e4fb2a","value":895731}},"28763562fc134c9ba738fbe1a2cf8735":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_68c97fb363a548a8bdb17a99b534ff23","placeholder":"​","style":"IPY_MODEL_2c6c95e178174383a644d44ff659ceac","value":" 896k/896k [00:00&lt;00:00, 10.5MB/s]"}},"25f758d718a94f46a731107b5617d927":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"482792b7827d475fab977a2b85bb2f30":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5eb309639cd64de0ab847ef4679114f8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5796140660234497aebc662697065c5f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ca5fcbb25da4a0cb7057c4207e4fb2a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"68c97fb363a548a8bdb17a99b534ff23":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c6c95e178174383a644d44ff659ceac":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"02ae3faaefbe4e739f526b90130615c0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_22609aa3f9224efda0a0878269121238","IPY_MODEL_fb5e4f0e37824f939b6324e7554f948c","IPY_MODEL_4fb8a8c845294b9f9a026ab3d3b2592f"],"layout":"IPY_MODEL_6d690bbbe91a4a509c6ad433f4ebd1de"}},"22609aa3f9224efda0a0878269121238":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed4f67cc05c24b4e98816516e8e94232","placeholder":"​","style":"IPY_MODEL_2fdfaea69e3a48868e8ef9e15f28a206","value":"config.json: 100%"}},"fb5e4f0e37824f939b6324e7554f948c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_df4efeef2f4a4378a972c0597f7dcf78","max":1516,"min":0,"orientation":"horizontal","style":"IPY_MODEL_56372d9c75a542668125e46451e13d10","value":1516}},"4fb8a8c845294b9f9a026ab3d3b2592f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6a2216f07b334e66a5bd8ef45c65db97","placeholder":"​","style":"IPY_MODEL_6b5112f977eb40f5bfe46a17972baf45","value":" 1.52k/1.52k [00:00&lt;00:00, 26.1kB/s]"}},"6d690bbbe91a4a509c6ad433f4ebd1de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed4f67cc05c24b4e98816516e8e94232":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2fdfaea69e3a48868e8ef9e15f28a206":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"df4efeef2f4a4378a972c0597f7dcf78":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"56372d9c75a542668125e46451e13d10":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6a2216f07b334e66a5bd8ef45c65db97":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b5112f977eb40f5bfe46a17972baf45":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"68e6f057b6ca493889b0ea9952853503":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_40e9a6d0cb1c47afa77933f6e2c43c75","IPY_MODEL_136a3c5410444a8a8d328fc1207f4baf","IPY_MODEL_19f320ec65ba4be1bc642c6cb53b3f12"],"layout":"IPY_MODEL_22bd89d776bb4c299837429d954dc658"}},"40e9a6d0cb1c47afa77933f6e2c43c75":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6e4d4db8c0f341bcad73a8dbaa952193","placeholder":"​","style":"IPY_MODEL_74980085741d46bcb9269308cdaad056","value":"pytorch_model.bin: 100%"}},"136a3c5410444a8a8d328fc1207f4baf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_95e7d84bfdb34a26b151fc73656d0c86","max":1493194721,"min":0,"orientation":"horizontal","style":"IPY_MODEL_34cc859c78ba4609a507db685791c3fe","value":1493194721}},"19f320ec65ba4be1bc642c6cb53b3f12":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7fad1bd498784486b4f1e43989f89dce","placeholder":"​","style":"IPY_MODEL_984dfd52c8d345a79449a687f24919d5","value":" 1.49G/1.49G [01:31&lt;00:00, 16.1MB/s]"}},"22bd89d776bb4c299837429d954dc658":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e4d4db8c0f341bcad73a8dbaa952193":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74980085741d46bcb9269308cdaad056":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"95e7d84bfdb34a26b151fc73656d0c86":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"34cc859c78ba4609a507db685791c3fe":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7fad1bd498784486b4f1e43989f89dce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"984dfd52c8d345a79449a687f24919d5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7020335,"sourceType":"datasetVersion","datasetId":4036910},{"sourceId":7141399,"sourceType":"datasetVersion","datasetId":4121848}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install sentencepiece -q\n!pip install transformers datasets pandas scikit-learn-q\n!pip install accelerate -U -q\n!pip install sacremoses -q\n!pip install transformers -q","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cTOmjtWnX2sg","outputId":"6f9999ed-a193-46ab-b18f-2f90efeae846","execution":{"iopub.status.busy":"2023-12-19T18:17:08.925810Z","iopub.execute_input":"2023-12-19T18:17:08.926169Z","iopub.status.idle":"2023-12-19T18:17:47.565934Z","shell.execute_reply.started":"2023-12-19T18:17:08.926140Z","shell.execute_reply":"2023-12-19T18:17:47.564784Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.36.0)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.0.3)\n\u001b[31mERROR: Could not find a version that satisfies the requirement scikit-learn-q (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for scikit-learn-q\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"**These lines import necessary libraries including Pandas for data manipulation, PyTorch for deep learning, components from the Transformers library for using FlauBERT, and scikit-learn tools for data preprocessing and dataset splitting.**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import FlaubertTokenizer, FlaubertForSequenceClassification, Trainer, TrainingArguments\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Load the training data\ntraining_data_path = '/kaggle/input/aaaaaaaaa/augmented_training_data2(2).csv'\ntraining_data = pd.read_csv(training_data_path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this section of the code, we are performing several key preprocessing steps to prepare the dataset for training with the FlauBERT model:\n\nTokenizer Initialization:\n\n* We initialize the FlauBERT tokenizer, specifically the 'flaubert/flaubert_large_cased' variant. This tokenizer is crucial for processing French textual data. It converts raw text sentences into a machine-readable format, known as tokens. These tokens are numerical representations of text segments that the FlauBERT model can understand and analyze.\n\nEncoding Difficulty Levels:\n\n* we use using scikit-learn's LabelEncoder to transform the difficulty labels in the dataset from textual to numerical form. Since machine learning models inherently work with numbers, encoding categorical labels into numbers is an essential step. This process assigns a unique integer to each level of difficulty.\n\nTokenizing Sentences and Encoding Labels:\n\n* The sentences from the dataset are tokenized using the initialized FlauBERT tokenizer. This step involves breaking down each sentence into tokens and ensuring that they are of uniform length, achieved by truncating longer sentences and padding shorter ones. The maximum length is set to 512 tokens, aligning with the model's input size requirements.\nSimultaneously, we convert the encoded difficulty labels into a list format, aligning them with the tokenized sentences. This alignment is crucial for supervised learning, where each input (tokenized sentence) is associated with a corresponding output label (difficulty level).\n\nThrough these steps, we are ensuring that the data is in the correct format and ready for training with the FlauBERT model, setting the stage for effective machine learning on language data.","metadata":{}},{"cell_type":"code","source":"# Initialize the FlauBERT tokenizer\ntokenizer = FlaubertTokenizer.from_pretrained('flaubert/flaubert_large_cased')\n\n# Encode the difficulty levels\nlabel_encoder = LabelEncoder()\ntraining_data['encoded_labels'] = label_encoder.fit_transform(training_data['difficulty'])\n\n# Tokenize the sentences and encode the labels\ntrain_encodings = tokenizer(training_data['sentence'].tolist(), truncation=True, padding=True, max_length=512)\ntrain_labels = training_data['encoded_labels'].tolist()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this portion of the code, we are defining a custom dataset class named `FrenchDifficultyDataset`, which is tailored for use with PyTorch, particularly for handling the data we've prepared for our FlauBERT model:\n\n1. **Defining the Dataset Class:**\n   - We create a class `FrenchDifficultyDataset` that inherits from `torch.utils.data.Dataset`. This class is specifically designed to handle our tokenized sentences and their corresponding difficulty labels.\n\n2. **Initialization Method (`__init__`):**\n   - In the initializer (`__init__`), we take two arguments: `encodings` and `labels`. The `encodings` are the tokenized representations of our sentences, and `labels` are the corresponding difficulty levels that we have encoded earlier.\n   - We assign these `encodings` and `labels` to instance variables within the class so that they can be accessed by other methods in the class.\n\n3. **Get Item Method (`__getitem__`):**\n   - The `__getitem__` method is defined to facilitate the retrieval of data samples by index. For a given index `idx`, this method returns a dictionary where each key-value pair corresponds to input features and their values, along with the associated label for that data point.\n   - We convert each item in the `encodings` and the `labels` to PyTorch tensors. This conversion is essential because PyTorch models expect data in the form of tensors.\n\n4. **Length Method (`__len__`):**\n   - The `__len__` method returns the total number of samples in the dataset. This is simply the length of the `labels` list, as each label corresponds to one encoded sentence.\n\nBy defining this `FrenchDifficultyDataset` class, we are effectively packaging our preprocessed data (both the input encodings and the output labels) into a format that is compatible with the PyTorch framework, particularly for use in training and evaluation loops. This class will enable us to seamlessly integrate our dataset with PyTorch's data loading and batching utilities.","metadata":{}},{"cell_type":"code","source":"# Prepare the dataset\nclass FrenchDifficultyDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this part of the code, we are splitting our prepared dataset into training and validation sets, an essential step in the machine learning workflow:\n\n1. **Creating the Dataset Instance:**\n   - First, we instantiate our custom `FrenchDifficultyDataset` class using `train_encodings` and `train_labels`. This dataset includes the tokenized sentences and their corresponding difficulty levels. The `FrenchDifficultyDataset` object encapsulates our data in a format that's compatible with PyTorch, facilitating easier data handling during model training.\n\n2. **Splitting the Dataset:**\n   - We then use the `train_test_split` function from scikit-learn to divide our dataset into training and validation sets. This function is a standard utility in machine learning used to evaluate the performance of a model on unseen data.\n   - By setting `test_size=0.1`, we allocate 10% of our data for validation and the remaining 90% for training. The validation set is crucial for tuning the model and checking for issues like overfitting, where the model performs well on the training data but poorly on new, unseen data.\n\nBy completing this step, we ensure that we have a well-defined training set to fit our model and a separate validation set to evaluate its performance. This division is critical for developing robust machine learning models that generalize well to new data.","metadata":{}},{"cell_type":"code","source":"# Split the training data\ntrain_dataset, val_dataset = train_test_split(FrenchDifficultyDataset(train_encodings, train_labels), test_size=0.1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this line of code, we're setting up the FlauBERT model for our specific sequence classification task:\n\n1. **Loading the FlauBERT Model:**\n   - We use the `FlaubertForSequenceClassification.from_pretrained` method to load a pre-trained FlauBERT model. This method is ideal for loading models that have been pre-trained on a large corpus of data and are well-suited for fine-tuning on specific tasks like ours.\n   - The model variant we choose is `'flaubert/flaubert_large_cased'`, which is a large-sized FlauBERT model that respects the case (uppercase/lowercase) of the input text. This variant is particularly effective for understanding the nuances in language.\n\n2. **Configuring for Sequence Classification:**\n   - We specify `num_labels=len(label_encoder.classes_)` in the model's configuration. This tells the model the number of distinct labels (or classes) it needs to predict. The `num_labels` should match the number of different difficulty levels in our dataset, which we determine by the length of `label_encoder.classes_`.\n   - By setting the `num_labels`, we are effectively tailoring the FlauBERT model for our classification task, ensuring it outputs predictions corresponding to our encoded difficulty levels.\n\nThrough this process, we configure the FlauBERT model to understand the specific requirements of our task, making it ready for subsequent training with our dataset. This step is crucial in adapting powerful, pre-trained models to specialized tasks with relatively less effort and time compared to training a model from scratch.","metadata":{}},{"cell_type":"code","source":"# Load the FlauBERT model\nmodel = FlaubertForSequenceClassification.from_pretrained('flaubert/flaubert_large_cased', num_labels=len(label_encoder.classes_))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this section of the code, we're defining the settings and hyperparameters for training our FlauBERT model:\n\n1. **Setting Up Training Arguments:**\n   - We use the `TrainingArguments` class from the Hugging Face Transformers library to configure various aspects of the training process. This configuration will be passed to the Trainer object later on.\n\n2. **Configuration Details:**\n   - `output_dir='./results'`: We specify a directory where the training outputs (like model checkpoints) will be saved. This is useful for keeping track of training results and for potential model recovery in case of interruptions.\n   - `num_train_epochs=3`: This sets the number of training epochs. An epoch is one complete pass through the entire training dataset. We choose to train for three epochs.\n   - `per_device_train_batch_size=8`: This determines the batch size for training on each device (like a GPU or CPU). A smaller batch size can help reduce memory usage but might affect training speed and convergence.\n   - `warmup_steps=500`: Warmup steps are used to gradually ramp up the learning rate at the beginning of training. This can help in stabilizing the training process and is often beneficial for fine-tuning.\n   - `weight_decay=0.01`: This is a regularization parameter that helps prevent the model from overfitting to the training data. It adds a penalty for larger weights in the model.\n   - `logging_dir='./logs'`: Specifies where to save logs generated during training. This is helpful for monitoring the training process and debugging.\n   - `logging_steps=10`: Determines how often to log training information. In this case, we log every 10 steps.\n   - `save_strategy=\"no\"`: We're disabling saving model checkpoints at the end of each epoch, which can be useful for saving disk space and speeding up training.\n   - `save_steps=1e9`: Sets a very high number of steps for saving the model, effectively disabling periodic checkpoint saves due to the high threshold.\n\nBy configuring these training arguments, we tailor the training process to our specific needs and computational constraints. These settings are crucial for efficient and effective training of the model on our dataset.","metadata":{}},{"cell_type":"code","source":"# Define the training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=3,\n    per_device_train_batch_size=8,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n        save_strategy=\"no\",  # Disable saving at the end of each epoch\n    save_steps=1e9\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this part of the code, we are initializing and conducting the training process for our FlauBERT model using the Hugging Face `Trainer` API:\n\n1. **Initializing the Trainer:**\n   - We create an instance of the `Trainer` class, which is a part of the Hugging Face Transformers library. This class simplifies the training process by abstracting many of the complex steps involved in training deep learning models.\n   - We pass the previously configured FlauBERT model to the `Trainer` as the `model` parameter.\n   - The `training_args` we defined earlier are also passed to the `Trainer`. These arguments provide the Trainer with our specific training configurations like the number of epochs, batch size, and logging details.\n   - `train_dataset` is provided as the dataset for training. This is the dataset that the model will learn from.\n   - `eval_dataset` is specified for evaluation. The model's performance will be periodically assessed on this dataset to understand how well it is learning and generalizing.\n\n2. **Training the Model:**\n   - We call the `train()` method on the Trainer instance. This method starts the training process of our model on the specified training dataset.\n   - During training, the model learns to classify the difficulty level of French sentences by adjusting its internal parameters based on the error between its predictions and the actual labels.\n   - The training process will automatically evaluate the model on the validation dataset and log the training progress as per the settings defined in `training_args`.\n\nBy using the `Trainer` API, we streamline the training process, making it more manageable and less error-prone. The API handles many underlying details like batch processing, gradient calculations, and model evaluations, allowing us to focus more on fine-tuning the training configuration and interpreting the results.","metadata":{}},{"cell_type":"code","source":"# Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n)\n\n# Train the model\ntrainer.train()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this final part of the code, we are focused on saving the trained model and loading the unlabelled test data:\n\n1. **Saving the Trained Model:**\n   - After training the FlauBERT model, we use the `save_pretrained` method to save it. This method ensures that all the model's parameters and configurations are stored correctly.\n   - We specify the path `/kaggle/working/flaubertlevrai-finetuned` as the location to save the model. This path is particularly relevant in the Kaggle environment, where `/kaggle/working` is a common directory used for output files. The model can be accessed from this directory for future use, like making predictions or further fine-tuning.\n   - The saved model includes the fine-tuned weights that have been adjusted to our specific task of predicting the difficulty level of French sentences.\n\n2. **Loading Unlabelled Test Data:**\n   - We load an unlabelled test dataset from the specified CSV file located at `/kaggle/input/trainin/unlabelled_test_data.csv`. This dataset is expected to contain French sentences for which we want to predict the difficulty levels.\n   - The data is loaded into a Pandas DataFrame `unlabelled_test_data` using the `read_csv` method. This DataFrame will be used to prepare the data for making predictions with our trained model.\n\nIn summary, these steps are crucial for finalizing the machine learning workflow. Saving the trained model allows us to reuse it without the need to retrain, and loading the unlabelled test data prepares us for the next step, which is typically to make predictions and evaluate the model's performance on real-world data.","metadata":{}},{"cell_type":"code","source":"# Save the model\nmodel.save_pretrained('/kaggle/working/flaubertlevrai-finetuned')\n\n# Load the unlabelled test data\nunlabelled_test_data_path = '/kaggle/input/trainin/unlabelled_test_data.csv'\nunlabelled_test_data = pd.read_csv(unlabelled_test_data_path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this segment of the code, we're processing the unlabelled test data to make it compatible with our trained FlauBERT model, and we're preparing a PyTorch dataset for the test data:\n\n1. **Preprocessing the Test Data:**\n   - We use the previously initialized FlauBERT tokenizer to process the sentences in the unlabelled test dataset. This involves tokenizing the sentences, similar to how we processed our training data.\n   - The `tokenizer` function is called with `truncation=True` and `padding=True` to ensure that all tokenized outputs have the same length, specified by `max_length=512`. This uniformity is essential for the model to process the data correctly.\n\n2. **Creating the Test Dataset Class:**\n   - We define a custom class `FrenchTestDataset` that inherits from `torch.utils.data.Dataset`. This class is tailored to handle the tokenized test data for PyTorch.\n   - The `__init__` method takes the tokenized data (`encodings`) as input and stores it in an instance variable.\n   - The `__getitem__` method allows us to retrieve a single tokenized instance from the dataset by index. This method will be used by PyTorch to iterate over the dataset during the prediction phase.\n   - The `__len__` method returns the total number of samples in the dataset, which is determined by the length of the `input_ids` in the encodings.\n\n3. **Instantiating the Test Dataset:**\n   - We create an instance of the `FrenchTestDataset` class using the `test_encodings`. This instance, `test_dataset`, is a PyTorch-compatible dataset containing our preprocessed test sentences.\n\nBy preparing the test dataset in this manner, we ensure that our unlabelled data is in the correct format for making predictions with the trained FlauBERT model. This step is vital for evaluating the model's performance on new, unseen data.","metadata":{}},{"cell_type":"code","source":"# Preprocess the test data\ntest_encodings = tokenizer(unlabelled_test_data['sentence'].tolist(), truncation=True, padding=True, max_length=512)\n\n# Prepare the test dataset\nclass FrenchTestDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        return item\n\n    def __len__(self):\n        return len(self.encodings['input_ids'])\n\ntest_dataset = FrenchTestDataset(test_encodings)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this final step of our machine learning workflow, we use our trained FlauBERT model to make predictions on the unlabelled test dataset and then decode these predictions back into human-readable labels:\n\n1. **Running Predictions:**\n   - We utilize the `predict` method of our `Trainer` object to perform predictions on the `test_dataset`. This method efficiently processes the dataset through our trained FlauBERT model to generate predictions.\n   - Our `test_dataset` is comprised of tokenized sentences from the unlabelled test data, formatted specifically to be compatible with the FlauBERT model.\n\n2. **Decoding the Predictions:**\n   - The model’s predictions are initially logits for each class (difficulty level). We use the `argmax` function to select the most probable class for each sentence.\n   - We then apply the `inverse_transform` method of the `LabelEncoder` to translate these numerical predictions back into their original categorical labels (like \"A1\", \"B2\", etc.). This step is crucial as it converts the model's output into an interpretable form.\n\nBy doing this, we are able to not only predict the difficulty levels of new sentences but also interpret these predictions in a meaningful way. This is an essential part of deploying a machine learning model, where we transform its numerical outputs into actionable insights.","metadata":{}},{"cell_type":"code","source":"# Run prediction\npredictions = trainer.predict(test_dataset)\npredicted_labels = label_encoder.inverse_transform(predictions.predictions.argmax(-1))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this final stage of our process, we're focusing on saving the predictions made by our model back to the unlabelled test dataset and then exporting this enriched dataset to a CSV file:\n\n1. **Appending Predictions to Test Data:**\n   - We add a new column, `'difficulty'`, to our `unlabelled_test_data` DataFrame. This column is filled with the `predicted_labels` we obtained from our model.\n   - By doing this, each sentence in the test dataset is now associated with a predicted difficulty level, effectively combining our original unlabelled data with the insights gained from the model.\n\n2. **Preparing the Data for Export:**\n   - We create a new DataFrame `una` by dropping the `'sentence'` column from `unlabelled_test_data`. The reason might be to focus on the predictions alone, or to conform to data privacy requirements by not exporting raw text.\n   - Dropping the sentence column helps in cases where we only need the model's output for further analysis or reporting.\n\n3. **Exporting to CSV:**\n   - We use the `to_csv` method to save the DataFrame `una` to a CSV file. This file is named `'letraduit.csv'` and is saved to the `/kaggle/working` directory, which is a standard directory for output files on Kaggle.\n   - The `index=False` parameter is used to prevent pandas from writing row indices into the CSV file, ensuring that the file contains only the data columns.\n\nThrough these steps, we're not only able to generate and understand the model's predictions but also export these results in a structured and accessible format. This is crucial for sharing our findings, conducting further analysis, or integrating them into larger systems or reports.","metadata":{}},{"cell_type":"code","source":"# Save the predictions to the unlabelled test data\nunlabelled_test_data['difficulty'] = predicted_labels\nuna = unlabelled_test_data.drop('sentence', axis=1)\nuna.to_csv('/kaggle/working/letraduit.csv', index=False)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["2a161d00be9445b18b20037654d312da","7842dccc547c47b7b7c9235b92e9b9cc","85c7a13d776f44958290e1ed063271b6","f6cf07f904fd424d88695d586b8d2cbf","db2b464bb6174df19771c9f80917d360","528c26b7f6a34ac6b33057e6a2732239","ffb4fab85eb243a4b00e59f6ba8bf216","4d7116354d584caa9581914424ad47e1","05a4af6769a94addb4647594f9bd340d","c8c3db074c98444681f7a96929b4ba8e","115dac200f474c5095555157653481de","e805a6d19bb043aaa205684986762bed","228b9673420b451bac630a7933262c27","50cf265ca2344bbe8a41af04289a9f3d","5c86dba6dfe94ad88e2b292b802e84e4","e991d282d3214ffdba3b659ecd8412a0","565d3b4b5d4a43ff8ca933d4216fe410","77e215d6bf314be88d213b0ccc3d3840","6360fedd21b64dc694dea7c85db38d76","3eb185b4bf034d5abd1ec8d9558f486a","93cdba5bcbbf4a17b5da785245a7d79f","1d2f787652a34bd58872002300ed6dbe","ec46ccd7fa9041b3812609079bf25227","40fc7737d0e84ac79f5a94512db208f3","1f93d426cba64c9088b1fa8bbf6066b5","28763562fc134c9ba738fbe1a2cf8735","25f758d718a94f46a731107b5617d927","482792b7827d475fab977a2b85bb2f30","5eb309639cd64de0ab847ef4679114f8","5796140660234497aebc662697065c5f","3ca5fcbb25da4a0cb7057c4207e4fb2a","68c97fb363a548a8bdb17a99b534ff23","2c6c95e178174383a644d44ff659ceac","02ae3faaefbe4e739f526b90130615c0","22609aa3f9224efda0a0878269121238","fb5e4f0e37824f939b6324e7554f948c","4fb8a8c845294b9f9a026ab3d3b2592f","6d690bbbe91a4a509c6ad433f4ebd1de","ed4f67cc05c24b4e98816516e8e94232","2fdfaea69e3a48868e8ef9e15f28a206","df4efeef2f4a4378a972c0597f7dcf78","56372d9c75a542668125e46451e13d10","6a2216f07b334e66a5bd8ef45c65db97","6b5112f977eb40f5bfe46a17972baf45","68e6f057b6ca493889b0ea9952853503","40e9a6d0cb1c47afa77933f6e2c43c75","136a3c5410444a8a8d328fc1207f4baf","19f320ec65ba4be1bc642c6cb53b3f12","22bd89d776bb4c299837429d954dc658","6e4d4db8c0f341bcad73a8dbaa952193","74980085741d46bcb9269308cdaad056","95e7d84bfdb34a26b151fc73656d0c86","34cc859c78ba4609a507db685791c3fe","7fad1bd498784486b4f1e43989f89dce","984dfd52c8d345a79449a687f24919d5"]},"id":"4mITw3y6Weeb","outputId":"f2d9cffb-fe23-4534-a198-22654c68e4fd","execution":{"iopub.status.busy":"2023-12-06T21:42:28.417755Z","iopub.execute_input":"2023-12-06T21:42:28.418070Z","iopub.status.idle":"2023-12-06T23:13:40.867102Z","shell.execute_reply.started":"2023-12-06T21:42:28.418039Z","shell.execute_reply":"2023-12-06T23:13:40.865984Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"111a442741d340149bf815a4e2750b01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading vocab.json:   0%|          | 0.00/1.56M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f6714fa691f4adf96017ee866cac98b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading merges.txt:   0%|          | 0.00/896k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a88dcd113a54c49bd7ee30301c21c2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/1.52k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9aaa49397d7b493eacc1447a519bf98d"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype):\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/1.49G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf92b173d49340f5a066b684dcd9782f"}},"metadata":{}},{"name":"stderr","text":"Some weights of FlaubertForSequenceClassification were not initialized from the model checkpoint at flaubert/flaubert_large_cased and are newly initialized: ['sequence_summary.summary.bias', 'sequence_summary.summary.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.1 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231206_214335-cqhe22cs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/razik18/huggingface/runs/cqhe22cs' target=\"_blank\">vocal-oath-16</a></strong> to <a href='https://wandb.ai/razik18/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/razik18/huggingface' target=\"_blank\">https://wandb.ai/razik18/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/razik18/huggingface/runs/cqhe22cs' target=\"_blank\">https://wandb.ai/razik18/huggingface/runs/cqhe22cs</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3240' max='3240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3240/3240 1:28:47, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>2.564500</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>2.408700</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>2.182400</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>2.180900</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.949000</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>2.156900</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>1.975700</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.963400</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>1.868800</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.844500</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>1.976700</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.800700</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>1.823400</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>1.693800</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.666300</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>1.601900</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>1.568300</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>1.592700</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>1.606600</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.722400</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>1.569900</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>1.555600</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>1.545100</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>1.690700</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.507400</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>1.530600</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>2.194100</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>1.489500</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>1.513600</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.530500</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>1.327000</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>1.491900</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>1.389300</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>1.322600</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>1.559800</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>1.382800</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>1.656700</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>1.268200</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>1.850400</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.571100</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>1.276300</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>1.353100</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>1.380700</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>1.271200</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>1.326900</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>1.305600</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>1.568000</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>1.320500</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>1.307400</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.583900</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>1.521300</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>1.268000</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>1.242900</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>1.635600</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>1.553000</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>1.132600</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>1.324600</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>1.181100</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>1.169900</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.428800</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>2.149000</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>1.268000</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>1.366200</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>1.201400</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>1.100400</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>1.386900</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>1.930300</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>6.381200</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>1.297800</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>1.332700</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>1.241600</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>1.368700</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>1.271800</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>1.221200</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>1.322000</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>0.982700</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>1.257100</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>1.247300</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>1.039900</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>1.310700</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>1.138400</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>1.058700</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>1.162700</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>1.052400</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>1.140600</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>1.234500</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>1.171600</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>1.188400</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>1.071900</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>1.233300</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>1.291300</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>1.347400</td>\n    </tr>\n    <tr>\n      <td>930</td>\n      <td>1.059600</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>1.061600</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.980700</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>1.075400</td>\n    </tr>\n    <tr>\n      <td>970</td>\n      <td>1.112500</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>0.950200</td>\n    </tr>\n    <tr>\n      <td>990</td>\n      <td>1.017800</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.876900</td>\n    </tr>\n    <tr>\n      <td>1010</td>\n      <td>0.959800</td>\n    </tr>\n    <tr>\n      <td>1020</td>\n      <td>0.978800</td>\n    </tr>\n    <tr>\n      <td>1030</td>\n      <td>0.995500</td>\n    </tr>\n    <tr>\n      <td>1040</td>\n      <td>0.759900</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>1.133100</td>\n    </tr>\n    <tr>\n      <td>1060</td>\n      <td>0.982600</td>\n    </tr>\n    <tr>\n      <td>1070</td>\n      <td>1.136900</td>\n    </tr>\n    <tr>\n      <td>1080</td>\n      <td>1.119700</td>\n    </tr>\n    <tr>\n      <td>1090</td>\n      <td>0.855800</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.838000</td>\n    </tr>\n    <tr>\n      <td>1110</td>\n      <td>1.007000</td>\n    </tr>\n    <tr>\n      <td>1120</td>\n      <td>0.745100</td>\n    </tr>\n    <tr>\n      <td>1130</td>\n      <td>0.846500</td>\n    </tr>\n    <tr>\n      <td>1140</td>\n      <td>1.030100</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>0.939000</td>\n    </tr>\n    <tr>\n      <td>1160</td>\n      <td>0.743200</td>\n    </tr>\n    <tr>\n      <td>1170</td>\n      <td>0.689300</td>\n    </tr>\n    <tr>\n      <td>1180</td>\n      <td>1.045900</td>\n    </tr>\n    <tr>\n      <td>1190</td>\n      <td>0.863700</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.781300</td>\n    </tr>\n    <tr>\n      <td>1210</td>\n      <td>0.789300</td>\n    </tr>\n    <tr>\n      <td>1220</td>\n      <td>0.723400</td>\n    </tr>\n    <tr>\n      <td>1230</td>\n      <td>0.602000</td>\n    </tr>\n    <tr>\n      <td>1240</td>\n      <td>0.667000</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>0.892800</td>\n    </tr>\n    <tr>\n      <td>1260</td>\n      <td>0.918600</td>\n    </tr>\n    <tr>\n      <td>1270</td>\n      <td>0.684700</td>\n    </tr>\n    <tr>\n      <td>1280</td>\n      <td>0.783300</td>\n    </tr>\n    <tr>\n      <td>1290</td>\n      <td>0.741700</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.858500</td>\n    </tr>\n    <tr>\n      <td>1310</td>\n      <td>0.761900</td>\n    </tr>\n    <tr>\n      <td>1320</td>\n      <td>0.781300</td>\n    </tr>\n    <tr>\n      <td>1330</td>\n      <td>0.783800</td>\n    </tr>\n    <tr>\n      <td>1340</td>\n      <td>0.706400</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>0.577900</td>\n    </tr>\n    <tr>\n      <td>1360</td>\n      <td>0.803400</td>\n    </tr>\n    <tr>\n      <td>1370</td>\n      <td>0.737900</td>\n    </tr>\n    <tr>\n      <td>1380</td>\n      <td>0.678900</td>\n    </tr>\n    <tr>\n      <td>1390</td>\n      <td>0.691600</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.879400</td>\n    </tr>\n    <tr>\n      <td>1410</td>\n      <td>0.763300</td>\n    </tr>\n    <tr>\n      <td>1420</td>\n      <td>0.671900</td>\n    </tr>\n    <tr>\n      <td>1430</td>\n      <td>0.790100</td>\n    </tr>\n    <tr>\n      <td>1440</td>\n      <td>0.811700</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>0.724900</td>\n    </tr>\n    <tr>\n      <td>1460</td>\n      <td>0.657400</td>\n    </tr>\n    <tr>\n      <td>1470</td>\n      <td>0.652800</td>\n    </tr>\n    <tr>\n      <td>1480</td>\n      <td>0.678900</td>\n    </tr>\n    <tr>\n      <td>1490</td>\n      <td>0.633500</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.791200</td>\n    </tr>\n    <tr>\n      <td>1510</td>\n      <td>0.600800</td>\n    </tr>\n    <tr>\n      <td>1520</td>\n      <td>0.689500</td>\n    </tr>\n    <tr>\n      <td>1530</td>\n      <td>0.654200</td>\n    </tr>\n    <tr>\n      <td>1540</td>\n      <td>0.621200</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>0.682400</td>\n    </tr>\n    <tr>\n      <td>1560</td>\n      <td>0.621800</td>\n    </tr>\n    <tr>\n      <td>1570</td>\n      <td>0.665300</td>\n    </tr>\n    <tr>\n      <td>1580</td>\n      <td>0.503500</td>\n    </tr>\n    <tr>\n      <td>1590</td>\n      <td>0.581700</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.753400</td>\n    </tr>\n    <tr>\n      <td>1610</td>\n      <td>0.600600</td>\n    </tr>\n    <tr>\n      <td>1620</td>\n      <td>0.657800</td>\n    </tr>\n    <tr>\n      <td>1630</td>\n      <td>0.405200</td>\n    </tr>\n    <tr>\n      <td>1640</td>\n      <td>0.616600</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>0.676300</td>\n    </tr>\n    <tr>\n      <td>1660</td>\n      <td>0.447700</td>\n    </tr>\n    <tr>\n      <td>1670</td>\n      <td>0.476200</td>\n    </tr>\n    <tr>\n      <td>1680</td>\n      <td>0.665000</td>\n    </tr>\n    <tr>\n      <td>1690</td>\n      <td>0.481700</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.567700</td>\n    </tr>\n    <tr>\n      <td>1710</td>\n      <td>0.494000</td>\n    </tr>\n    <tr>\n      <td>1720</td>\n      <td>0.475900</td>\n    </tr>\n    <tr>\n      <td>1730</td>\n      <td>0.455600</td>\n    </tr>\n    <tr>\n      <td>1740</td>\n      <td>0.513900</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>0.339000</td>\n    </tr>\n    <tr>\n      <td>1760</td>\n      <td>0.578900</td>\n    </tr>\n    <tr>\n      <td>1770</td>\n      <td>0.513000</td>\n    </tr>\n    <tr>\n      <td>1780</td>\n      <td>0.478800</td>\n    </tr>\n    <tr>\n      <td>1790</td>\n      <td>0.555800</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.467400</td>\n    </tr>\n    <tr>\n      <td>1810</td>\n      <td>0.372500</td>\n    </tr>\n    <tr>\n      <td>1820</td>\n      <td>0.477700</td>\n    </tr>\n    <tr>\n      <td>1830</td>\n      <td>0.435000</td>\n    </tr>\n    <tr>\n      <td>1840</td>\n      <td>0.523100</td>\n    </tr>\n    <tr>\n      <td>1850</td>\n      <td>0.469800</td>\n    </tr>\n    <tr>\n      <td>1860</td>\n      <td>0.370300</td>\n    </tr>\n    <tr>\n      <td>1870</td>\n      <td>0.544100</td>\n    </tr>\n    <tr>\n      <td>1880</td>\n      <td>0.449700</td>\n    </tr>\n    <tr>\n      <td>1890</td>\n      <td>0.459600</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.376400</td>\n    </tr>\n    <tr>\n      <td>1910</td>\n      <td>0.568600</td>\n    </tr>\n    <tr>\n      <td>1920</td>\n      <td>0.538800</td>\n    </tr>\n    <tr>\n      <td>1930</td>\n      <td>0.410800</td>\n    </tr>\n    <tr>\n      <td>1940</td>\n      <td>0.364500</td>\n    </tr>\n    <tr>\n      <td>1950</td>\n      <td>0.450100</td>\n    </tr>\n    <tr>\n      <td>1960</td>\n      <td>0.496000</td>\n    </tr>\n    <tr>\n      <td>1970</td>\n      <td>0.341400</td>\n    </tr>\n    <tr>\n      <td>1980</td>\n      <td>0.294500</td>\n    </tr>\n    <tr>\n      <td>1990</td>\n      <td>0.470600</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.429700</td>\n    </tr>\n    <tr>\n      <td>2010</td>\n      <td>0.404200</td>\n    </tr>\n    <tr>\n      <td>2020</td>\n      <td>0.373200</td>\n    </tr>\n    <tr>\n      <td>2030</td>\n      <td>0.381300</td>\n    </tr>\n    <tr>\n      <td>2040</td>\n      <td>0.408500</td>\n    </tr>\n    <tr>\n      <td>2050</td>\n      <td>0.531300</td>\n    </tr>\n    <tr>\n      <td>2060</td>\n      <td>0.360600</td>\n    </tr>\n    <tr>\n      <td>2070</td>\n      <td>0.525600</td>\n    </tr>\n    <tr>\n      <td>2080</td>\n      <td>0.273000</td>\n    </tr>\n    <tr>\n      <td>2090</td>\n      <td>0.344100</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.418600</td>\n    </tr>\n    <tr>\n      <td>2110</td>\n      <td>0.424400</td>\n    </tr>\n    <tr>\n      <td>2120</td>\n      <td>0.586900</td>\n    </tr>\n    <tr>\n      <td>2130</td>\n      <td>0.334100</td>\n    </tr>\n    <tr>\n      <td>2140</td>\n      <td>0.309100</td>\n    </tr>\n    <tr>\n      <td>2150</td>\n      <td>0.481300</td>\n    </tr>\n    <tr>\n      <td>2160</td>\n      <td>0.348600</td>\n    </tr>\n    <tr>\n      <td>2170</td>\n      <td>0.285300</td>\n    </tr>\n    <tr>\n      <td>2180</td>\n      <td>0.194800</td>\n    </tr>\n    <tr>\n      <td>2190</td>\n      <td>0.220500</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.213800</td>\n    </tr>\n    <tr>\n      <td>2210</td>\n      <td>0.215300</td>\n    </tr>\n    <tr>\n      <td>2220</td>\n      <td>0.308900</td>\n    </tr>\n    <tr>\n      <td>2230</td>\n      <td>0.235600</td>\n    </tr>\n    <tr>\n      <td>2240</td>\n      <td>0.281900</td>\n    </tr>\n    <tr>\n      <td>2250</td>\n      <td>0.184000</td>\n    </tr>\n    <tr>\n      <td>2260</td>\n      <td>0.233100</td>\n    </tr>\n    <tr>\n      <td>2270</td>\n      <td>0.171200</td>\n    </tr>\n    <tr>\n      <td>2280</td>\n      <td>0.113800</td>\n    </tr>\n    <tr>\n      <td>2290</td>\n      <td>0.150100</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.176900</td>\n    </tr>\n    <tr>\n      <td>2310</td>\n      <td>0.129100</td>\n    </tr>\n    <tr>\n      <td>2320</td>\n      <td>0.175000</td>\n    </tr>\n    <tr>\n      <td>2330</td>\n      <td>0.319100</td>\n    </tr>\n    <tr>\n      <td>2340</td>\n      <td>0.220200</td>\n    </tr>\n    <tr>\n      <td>2350</td>\n      <td>0.202400</td>\n    </tr>\n    <tr>\n      <td>2360</td>\n      <td>0.322600</td>\n    </tr>\n    <tr>\n      <td>2370</td>\n      <td>0.172900</td>\n    </tr>\n    <tr>\n      <td>2380</td>\n      <td>0.175000</td>\n    </tr>\n    <tr>\n      <td>2390</td>\n      <td>0.145300</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.329800</td>\n    </tr>\n    <tr>\n      <td>2410</td>\n      <td>0.206900</td>\n    </tr>\n    <tr>\n      <td>2420</td>\n      <td>0.254100</td>\n    </tr>\n    <tr>\n      <td>2430</td>\n      <td>0.169700</td>\n    </tr>\n    <tr>\n      <td>2440</td>\n      <td>0.367400</td>\n    </tr>\n    <tr>\n      <td>2450</td>\n      <td>0.197800</td>\n    </tr>\n    <tr>\n      <td>2460</td>\n      <td>0.190300</td>\n    </tr>\n    <tr>\n      <td>2470</td>\n      <td>0.179500</td>\n    </tr>\n    <tr>\n      <td>2480</td>\n      <td>0.202800</td>\n    </tr>\n    <tr>\n      <td>2490</td>\n      <td>0.256000</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.210600</td>\n    </tr>\n    <tr>\n      <td>2510</td>\n      <td>0.239400</td>\n    </tr>\n    <tr>\n      <td>2520</td>\n      <td>0.164500</td>\n    </tr>\n    <tr>\n      <td>2530</td>\n      <td>0.238500</td>\n    </tr>\n    <tr>\n      <td>2540</td>\n      <td>0.139800</td>\n    </tr>\n    <tr>\n      <td>2550</td>\n      <td>0.157600</td>\n    </tr>\n    <tr>\n      <td>2560</td>\n      <td>0.222500</td>\n    </tr>\n    <tr>\n      <td>2570</td>\n      <td>0.199200</td>\n    </tr>\n    <tr>\n      <td>2580</td>\n      <td>0.157100</td>\n    </tr>\n    <tr>\n      <td>2590</td>\n      <td>0.167900</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>0.221300</td>\n    </tr>\n    <tr>\n      <td>2610</td>\n      <td>0.124300</td>\n    </tr>\n    <tr>\n      <td>2620</td>\n      <td>0.220400</td>\n    </tr>\n    <tr>\n      <td>2630</td>\n      <td>0.300000</td>\n    </tr>\n    <tr>\n      <td>2640</td>\n      <td>0.098300</td>\n    </tr>\n    <tr>\n      <td>2650</td>\n      <td>0.067900</td>\n    </tr>\n    <tr>\n      <td>2660</td>\n      <td>0.140000</td>\n    </tr>\n    <tr>\n      <td>2670</td>\n      <td>0.192300</td>\n    </tr>\n    <tr>\n      <td>2680</td>\n      <td>0.172500</td>\n    </tr>\n    <tr>\n      <td>2690</td>\n      <td>0.162000</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>0.178100</td>\n    </tr>\n    <tr>\n      <td>2710</td>\n      <td>0.132600</td>\n    </tr>\n    <tr>\n      <td>2720</td>\n      <td>0.149100</td>\n    </tr>\n    <tr>\n      <td>2730</td>\n      <td>0.127900</td>\n    </tr>\n    <tr>\n      <td>2740</td>\n      <td>0.268700</td>\n    </tr>\n    <tr>\n      <td>2750</td>\n      <td>0.199700</td>\n    </tr>\n    <tr>\n      <td>2760</td>\n      <td>0.155800</td>\n    </tr>\n    <tr>\n      <td>2770</td>\n      <td>0.227000</td>\n    </tr>\n    <tr>\n      <td>2780</td>\n      <td>0.142000</td>\n    </tr>\n    <tr>\n      <td>2790</td>\n      <td>0.183600</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>0.085300</td>\n    </tr>\n    <tr>\n      <td>2810</td>\n      <td>0.136100</td>\n    </tr>\n    <tr>\n      <td>2820</td>\n      <td>0.197600</td>\n    </tr>\n    <tr>\n      <td>2830</td>\n      <td>0.200700</td>\n    </tr>\n    <tr>\n      <td>2840</td>\n      <td>0.240000</td>\n    </tr>\n    <tr>\n      <td>2850</td>\n      <td>0.119600</td>\n    </tr>\n    <tr>\n      <td>2860</td>\n      <td>0.204900</td>\n    </tr>\n    <tr>\n      <td>2870</td>\n      <td>0.185800</td>\n    </tr>\n    <tr>\n      <td>2880</td>\n      <td>0.136200</td>\n    </tr>\n    <tr>\n      <td>2890</td>\n      <td>0.175900</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>0.138500</td>\n    </tr>\n    <tr>\n      <td>2910</td>\n      <td>0.165400</td>\n    </tr>\n    <tr>\n      <td>2920</td>\n      <td>0.146300</td>\n    </tr>\n    <tr>\n      <td>2930</td>\n      <td>0.224300</td>\n    </tr>\n    <tr>\n      <td>2940</td>\n      <td>0.154700</td>\n    </tr>\n    <tr>\n      <td>2950</td>\n      <td>0.124500</td>\n    </tr>\n    <tr>\n      <td>2960</td>\n      <td>0.073900</td>\n    </tr>\n    <tr>\n      <td>2970</td>\n      <td>0.085300</td>\n    </tr>\n    <tr>\n      <td>2980</td>\n      <td>0.167700</td>\n    </tr>\n    <tr>\n      <td>2990</td>\n      <td>0.186300</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.185400</td>\n    </tr>\n    <tr>\n      <td>3010</td>\n      <td>0.166100</td>\n    </tr>\n    <tr>\n      <td>3020</td>\n      <td>0.071700</td>\n    </tr>\n    <tr>\n      <td>3030</td>\n      <td>0.084200</td>\n    </tr>\n    <tr>\n      <td>3040</td>\n      <td>0.102400</td>\n    </tr>\n    <tr>\n      <td>3050</td>\n      <td>0.151600</td>\n    </tr>\n    <tr>\n      <td>3060</td>\n      <td>0.132700</td>\n    </tr>\n    <tr>\n      <td>3070</td>\n      <td>0.078200</td>\n    </tr>\n    <tr>\n      <td>3080</td>\n      <td>0.150900</td>\n    </tr>\n    <tr>\n      <td>3090</td>\n      <td>0.136300</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>0.132600</td>\n    </tr>\n    <tr>\n      <td>3110</td>\n      <td>0.115800</td>\n    </tr>\n    <tr>\n      <td>3120</td>\n      <td>0.075700</td>\n    </tr>\n    <tr>\n      <td>3130</td>\n      <td>0.142800</td>\n    </tr>\n    <tr>\n      <td>3140</td>\n      <td>0.195200</td>\n    </tr>\n    <tr>\n      <td>3150</td>\n      <td>0.077900</td>\n    </tr>\n    <tr>\n      <td>3160</td>\n      <td>0.058600</td>\n    </tr>\n    <tr>\n      <td>3170</td>\n      <td>0.079700</td>\n    </tr>\n    <tr>\n      <td>3180</td>\n      <td>0.156900</td>\n    </tr>\n    <tr>\n      <td>3190</td>\n      <td>0.100200</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>0.299900</td>\n    </tr>\n    <tr>\n      <td>3210</td>\n      <td>0.066400</td>\n    </tr>\n    <tr>\n      <td>3220</td>\n      <td>0.139300</td>\n    </tr>\n    <tr>\n      <td>3230</td>\n      <td>0.074700</td>\n    </tr>\n    <tr>\n      <td>3240</td>\n      <td>0.114400</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}]}]}